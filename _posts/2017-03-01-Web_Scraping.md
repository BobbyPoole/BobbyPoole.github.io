In order to have good data to work with, you need to not only have good sources but also be able to extract, clean and prepare data so that it can be evaluated. While many sources that you get will have little to nothing to clean and come in excel files which are easy to extract information from, other information is much harder. Web scraping is the process of extracting data or other information from a web page. It could be done for anything from copying a list of contacts to mining websites in order to find the best price of a particular item. In today's case we are scraping Indeed's web page in order to predict the expected earnings of an individual given the specific data science posting on Indeed's website.

Data and Assumptions:
For this data set I scraped hundreds of pages of Indeed's site looking at job postings for places that have the most data science jobs in the country. My scrape included the name of the city or town, the job title, the company, the salary and the job description. I then took the nearly 12,000 jobs I scraped and removed repeated listings (advertised listings mostly) so that the data would not lean unfairly toward a company who may have been listed multiple times but had only one opening. I then took the remaining listings and reduced it further to only include listings which had a salary and then took those remaining listings and predicted whether that specific job paid more than average salary of the remaining listings.

Some of the concerns I had doing this boiled down to the use of Indeed's data as well as the ultimate scarceness of information made available to me. While I was able to get thousands of postings, only 2,695 postings were unique. Then, of those unique postings only 161 had a salary figure. Finally, of the 161 only 137 had a salary for a year or a month. Those last 24 postings had to be discarded due to the fact postings that had an hourly, daily or weekly salary were likely temporary short term jobs and thus could not be assumed to last a year and would be difficult to compare against jobs which were either full time or at least went for one year. Based on the fact less than 1% of the scraped data could be used, it probably would have been better to use another site to ensure I had a larger data set to work with. Regardless, while this exercise would be improved from a modeling standpoint with a more robust data set, I can still show the intricacies of web scraping and cleaning a web scraped based data frame.

Web Scraping:
The initial scraping of the webpage is probably one of the more simple (though time consuming) parts of the job. The only difficulty is figuring out the pattern the URL is using so that the pages can be scraped properly. In my case, I needed to create a URL template which is essentially a URL formula that needs to be followed repeatedly for the scrape to be executed properly. In this case, it required Indeed's URL, an area of the country and a page number that tells the website the page/listings this search is on.
---
Once the pages are scraped, the information is just floating around so you need to create lists for the information to go to. Additionally, you need to set up the proper ways to pinpoint the information you are looking for in the webpage. For instance, if I'm trying to find the company name, I have to initially inspect the original webpage in order to see how the webpage was written so the information was placed where it was. For a company, I need to find a place where the class is equal to company, strip the information and add it to the list I have just created called company. That 'except' piece right below it is there so that in the event there is no company the data frame will still collect the data properly without disrupting my collection process or my data set.
--

Data Cleaning
Remember, this is all just to have a data frame. We still have to clean it. Before I start doing a lot of the more intricate stuff, there are two important things I need to do. First, I need to get rid of all jobs that are duplicates because most of the listings are asking for one person and including a listing multiple times compromises the integrity and accuracy of the data collected. Second, I need to save my data frame. The reason for that is in the event something goes horribly wrong and I lose my data or something happens while I'm working on it, I want to be sure that I have all of my information there so I can restart without an issue  and not waste a ton of time going through another scraping process which depending on how big of a scrape you are doing can take hours.
--

So I see that while I have 2,695 rows of data, only 161 of them have salary information, I cannot use the other 2,534 rows due to the fact that I have no information on that company's salary and thus would make it impossible for me to predict the salary of the particular job in question.

I then looked at my remaining data and noticed something that was concerning. There were a number of postings that had hourly, daily or weekly pay rates which I needed to decide whether to convert them to salary for a year or not. The issue with doing so is those jobs tend to be short term jobs which last a few months and are not comparable to longer term contract jobs or full time jobs. Ultimately, I decided to drop them due to the fact that comparing a short term job was not going to be level with comparing it to longer term or full time jobs.
--

Of those remaining 137 jobs, 37 had a monthly rate listed. Additionally, many of the listings had a range of salaries listed. So, I now needed to convert the monthly salary to a yearly one and change this salary range to a specific number. First, I created a label so I knew which listings had monthly salaries and which had yearly salaries. Second, I removed the year, month, commas associated in the salary and the dollar sign so I was left with either  a number or a range of numbers. Third, for the salaries with a range, I split them in order to create two numbers and made a scenario which said if it was one number, leave it, if not take the average of the two numbers. Finally, I made the salary reflective for the year by multiplying the salaries which were monthly. The chart below shows the distribution of those salaries once the cleaning was done.

Then I went and tackled the location. While Indeed left me with a number of jobs within the cities I originally selected, it also left me with many other areas just outside of the major metropolis. Consequently, I needed to convert these towns into the closest city available in order to reduce the number of distinct places so that we do not have too many features. Having a model with too many features causes the model to be over fit because it is trying to predict too accurately with the data it does have which means when it is given other data to predict the model will not be a good representation more broadly of what the model should look like. The downside to this is that it is different taking a job in Northbrook, Illinois  which is right outside of Chicago then it is in Chicago which can make grouping the two places a quick generalization since they may have vastly different rents and incomes needed to survive along with other things that one might consider when moving like crime, school district and transportation among other things that may be considered.

The descriptions were essentially the first few words of the job description of each posting. While each job has a full description, the scrape only revealed the first few words so it doesn't encapsulate the full description of each job. Having the whole description would give better insight into technologies companies might use or qualities they find important which could be used as better predictors of which jobs are higher paying relative to other ones. For each of the description words, I am removing all words which contain '^\w' and then splitting each word by the space and putting them in a separate list so that I can count how many of each words occurred. I followed the same thought process when dealing with the job title. After which, I picked three most common words in each in order to use them as predictors for whether or not they helped figure out if they gave you a bump in salary.

So, with all that done, we now need to scale down and drop all the unnecessary columns so that we can run a model on this data. When completed, the features of my model should be the nine cities (since I am using a logistics regression I need to pull out one city so the model will run properly), three words that were used for the job title and three words used in the description.

Now, I am going to predict for whether or not the specific job posting is higher or lower than the average accumulated from my data set. Before I start I check to see what percent fall under the average that was set.

For the data set, the percent that is under the average is 57% so if my model returns an  accuracy of less than 57% than I did not do a good enough job in predicting which jobs would pay more than the average. Ultimately, my model returned an accuracy score of 71.43%.

Today I went through and did one of the hardest things there is to do in data science, webscraping. Essentially webscraping is pulling specific information from many webpages in order to extract data and ultimately be able to use it for a statistical analysis.

I went through and scraped Indeed to find job posting information about data scientists. I specifically was looking at title, location, salary, job description and company. I went through and extracted hundreds of pages worth of data from 10 cities. Unfortunately, because I needed this data to have a salary and a location, my original number of 12000 jobs was slashed down to 137.

Though the website was scraped and I had my dataset, my work was nowhere near done. I then had to clean the data set. Specifically, I needed to extract the words out of the title and description of my data in order to create variables for common words to see if certain titles or skills needed meant a higher pay. So I dummified wrods like scientist and senior to see if they were better predictors of a higher salary. 

Then, I removed the dash and dollar sign in salary and averaged the range that was given so that I had 1 value to go off of for each data point. Finally, I removed the state and zipcode so I could look exclusively at the state.

With all that done, I ran a logistic regression and had an accuracy score of about .91. I also ran an ROC curve which yieled a score of .89.

Asking someone what makes a good movie could get you many different answers. Some might tell you that certain actors or directors will make the movie a standout. Others might point to certain types of films that consistently meet their needs like comedies or horror films. While there are elements that can help lift a movie from mediocrity to relevance, is it possible that certain elements can allow us to know whether or not a film is going to do well before we see a second of the movie? The goal today is to scrape IMDB for the top 250 films in order to figure out good films from the bad films.

Risks and Assumptions:
For this dataset, I used the number of votes each film received, the genre, key words associated with each film, when it was released, the length of the film and where each film was shot in order to determine which films were good films. By only including those elements as features, I excluded other potentially important elements. First, I did not include the general budget of each film as well as the amount that went into marketing the film. Second, I did not include which production companies were associated with each film. Third, I did not include important individuals like writers, actors, producers and directors in the films. Having those elements included could potentially help answer some important questions that can help predict if a movie is good. Does the presence of a famous or popular actor help? Does going above a certain budget end up having diminishing returns when it comes to ratings?

Additionally, since I am only doing the top 250 films, we are only looking at highly rated movies for this model. While we could be better served by having poorly rated movies in the dataset in order for the model to better be able to differentiate good movies from bad movies, this still allows us to be differentiate good and lesser movies relative to the films that we have used for this dataset.

Data Acquisition:
I started by importing IMDB from IMDB pie in order to get the list of the top 250 movies. I am doing this so that when I scrape the IMDB page for the top films I have a data frame already constructed. Next, I used BeautifulSoup to scrape the IMDB page to get the features I need for my model. Once you scrape you'll want to save the file in an assets folder. You want to make sure you do this after any scrape because in the event something happens while you play around with the data, you don't want to spend time rescraping a page. I did 250 films so the scrape didn't take too long, but say you wanted to do it for 25,000 films and you didn't save, you would lose ample time over something as minuscule as saving.

After saving your BeautifulSoup, you reload the file and you have to redefine BeautifulSoup. Think of it this way, when you save something that utilizes Beautiful Soup, your soup goes cold and cannot be consumed unless it is 'reheated'. In order to 'reheat' it, you have to define it as an array so that it is possible to extract all of the necessary elements you need from it. Next, you are going to start creating lists that your data will go in. Then, you are going to look back at the original page you scraped in order to figure out where in the page is the specific element you are seeking out. So for instance, I found runtimes of the films where the page uses time and has the itemprop labeled as duration. So this code will find every instance where this applies and adds it. I also have to include an element where if it isn't there, it won't include it because without that extra piece of information, I could get an error and that will disrupt and complicate the whole process. I then do the same for the genre, the country, the release date, the key words of the film and the rating of the film. Finally, I turn my lists into columns and add them to my original data frame.
![Image Name](../images/Runtime.Finder.jpg)

Once again, you should save the data frame for the same reasons I mentioned with the scraping, you don't want to go through all that work for something to happen while you're cleaning your data which would require you to rescrape and cost you hours of time.

Data Cleaning:

I now start the process of cleaning. For the most part, it's not too bad. The year needs to simply be changed from a string to an integer so it can be used in the model. Next, I tackle the release date. My goal here is extracting the month, This is a bit more complicated due to the fact that the date does not have a uniform way of being implemented. Some have the day, month and year while some just have the month and the year. So, I split up each term and created a list which would count the length of each item. In doing so, I could potentially find a pattern as to how IMDB set up the date so I can easily extract the month. Once I see that the length is anywhere from 2 to 5 items, I create a new list for the month as well as an if/else situation which will extract the specific item based on the the number of items in that particular film. For instance, when the length is 2 I look for the first item and add it to the new list I call month. For the most part, I am able to get the months I need with the exception of four titles where I got a year. For each of them, I used the year in order to find out which film they were so I could look up the title and then impute the correct month.

Next, I found which genres were associated with each titles. So each title has one or more genres associated with it.
-
In order to make this feature useful to my model, I need to separate all of the different types of genres and create features for each genre, if that film has particular genres, it gets a 1 and if not, it'll get a 0. So for the example I showed you a few lines up, that particular film would have a 1 for crime as well for drama in order to denote that the film is a crime and drama movie. All other genres for that film like adventure and thriller would be given a zero since those are not genres that represent that film.

Next, I found the countries where each film was shot. The countries feature was set up in the same way as genres so I used the same approach to find each country. The only difference was I removed several countries which only had one film that was shot in a particular country due to the fact that having only one representative from each country made it hard to determine how much that area may influence how good or bad film a film is. This was partially why I chose not to do actors or directors because there would have been many examples where an actor or director is involved once or twice and it would be hard to consider the impact in using the actor, director or country that is seen only once in this data set.

Finally, I picked out the key words of each film. Again, this follows the exact same methodology as the previous two features. The only difference is in this example I kept words used in at least five of the 250 films in order to have a value large enough where I could see the effect of the word while also limiting the the number of features so I will not over fit my model.

EDA, Model:

Once done, I was able to dummify my features in order to for each of the features, month, key words, and genre to be properly used.

One of the more interesting findings was how the number reviews was distributed. On the chart below, you can see how older movies (before 1990) had far fewer votes than more recent films.

For my model, I split the films into two categories, one for films with ratings lower than the collective median over the 250 films and the other under. The goal was for an accuracy of 66% based on the baseline of the data given. I ran both a decision tree and a random forest in order to see which model would be more accurate. I trained my model using about 30% of the data and tested it on the remaining 70%. Ultimately, the decision tree had an accuracy of 82.8% accuracy plus or minus 4.4 points while the random forest had an accuracy of  77.4% plus or minus 1.3 points. So the decision tree is more accurate but the random forest has less variance.

I then wanted to find out how much each feature weighed when it came to the mode. The number of votes and the year the film came out accounted for 29% and 11% respectively with months like October and November as well as genres like fantasy, thriller and drama having some impact. The problem with this is that in order to predict if something is good, then I cannot have future information. Because the votes come after the film is released, the number of votes is a feature that ruins the prediction element of the model and must be removed for the sake of fairness.

In redoing the model, the decision tree got only a 65% accuracy plus or minus 6.1 points while the random forest had a 70.9% accuracy plus or minus 6 points. So in actuality the random forest is actually the better model for this, I also wanted to see how important certain features became once I removed an element like number of votes that accounted for such a high percent of the importance. In doing so we see that year moves up to 15.7% while the next closest is October with less than 5%. This to me shows that there is a recency bias with films on IMDB. Essentially the higher up movies are generally the ones that have been seen more recently made which makes sense because IMDB bases their top 250 films not solely on rating but on other factors as well. Additionally, remember the chart I showed you a few paragraphs back about how many votes movies from before 1990 vs 1990 or later. It reaffirms the point that modern movies have much more pull in IMDB's rankings vs. older films.

Conclusion:

There's a lot to unpack here. While using the top 250 films, location, genre, and release data gave us an okay result given that we beat the baseline but are still within the margin of error, it does speak to having a larger, more diverse dataset can add to the model. All of the films that were used were rated from an 8 to a 9.3 on a 10 point scale. Had more films been included lower down, we may have had a better time differentiating between good and bad movies as opposed to good and great movies. Additionally, it would be important to include some more features. While actors and directors inclusion to features may help, I am curious to see how movie studios and their budgets influence the overall rating that these films get from the public at large. Adding these elements will help us figure out what makes a good film from a bad film.

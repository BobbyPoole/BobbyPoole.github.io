Asking an individual what makes a good movie could get you many different answers. Some individuals might tell you that certain actors or directors will make the movie a standout. Others might point to a specific genre to point out their preferences for certain types of films. But, how can we figure out before a single review is given whether or not a movie can be considered good. The goal today is to scrape IMDB for the top 250 films in order to figure out the good ones from the bad ones.

Risks and Assumptions:
For this dataset, I used the number of votes each film received, the genre, key words associated with each film, when it was released, the length of the film and where each film was shot. In only including those elements as features, I excluded other elements. First, I did not include the general budget of each film as well as the amount that went into marketing the film. Second, I did not include which production companies were associated with each film. Third, I did not include important individuals like writers, actors, producers and directors in the films.

Additionally, since I am only doing the top 250 films, we are only looking at highly rated movies for this model. While we could be better served by having more poorly rated movies encompassed in the dataset in order for the model to better be able to differentiate good movies from bad movies, this still allows us to be able to compare good movies from bad movies.

Data Acquisition:
I started by importing IMDB from IMDB pie in order to get the list of the top 250 movies. I am doing this so that when I scrape the IMDB page for the top films I have a data frame already constructed. Next, I used BeautifulSoup to scrape the IMDB page to get the features I need for my model. Once you scrape you'll want to save the file in an assets folder. You want to make sure you do this after any scrape because in the event something happens while you play around with the data, you don't want to spend time rescraping a page. I did 250 films so the scrape didn't take too long, but say you wanted to do it for 25,000 films and you didn't save, you would lose ample time over something as minuscule as saving.

After saving your BeautifulSoup, you reload the file and you have to redefine BeautifulSoup. Think of it this way, when you save something that utilizes Beautiful Soup, your soup goes cold and cannot be consumed unless it is 'reheated'. In order to 'reheat' it, you have to define it as an array so that it is possible to extract all of the necessary elements you need from it. Next, you are going to start creating lists that your data will go in. Then, you are going to look back at the original page you scraped in order to figure out where in the page is the specific element you are seeking out. So for instance, I found runtimes of the films where the page uses time and has the itemprop labeled as duration. So this code will find every instance where this applies and adds it. I also have to include an element where if it isn't there it doesn't include it because without that extra piece of information, I could get an error. I then do the same for the genre, the country, the release date, the key words of the film and the rating of the film. Finally, I turn my lists into columns and add them to my original data frame.
![Image Name](../images/Runtime.Finder.jpg)

Once again, you should save the data frame for the same reasons I mentioned with the scraping, you don't want to go through all that work for something to happen while you're cleaning your data which would require you to rescrape and cost you hours of time.

Data Cleaning:

I now start the process of cleaning. For the most part, it's not too bad. The year needs to simply be changed from a string to an integer so it can be used in the model. Next, I tackle the release date. My goal here is extracting the month, This is a bit more complicated due to the fact that the date does not have a uniform way of being implemented. Some have the day, month than year, some just have the month and the year. So, I split up each term and created a list which would count the length of each item. In doing so, I could potentially find a pattern as to how IMDB set up the date so I can easily extract the month. Once I see that the length is anywhere from 2 to 5 items, I create a new list for the month as well as an if/else situation which will extract the nth term based on the the number of items in that particular film. For instance, when the length is 2 I look for the first item and add it to the new list I call month. For the most part, I am able to get the months I need with the exception of four titles where I got a year. For each of them, I used the year in order to find out which film they were so I could look up the title and then impute the correct month.

Next, I found which genres were associated with each titles. So each title has one or more genres associated with it.
-
In order to make this feature useful to my model, I need to separate all of the different types of genres and create features for each genre, if that film has particular genres, it gets a 1 and if not, it'll get a 0 . So for the example I showed you a few lines up, that particular film would use a 1 for crime as well for drama in order to denote that the film is a crime and drama movie. All other genres for that film like adventure and thriller would be given a zero since those are not genres that represent that film.

Next, I found the countries where each film was shot. Countries was set up in the same way as genres so I used the same approach to find each country. The only difference was I removed several countries which only had one film that was shot in a particular country due to the fact that having only one representative from each country made it hard to determine how much that influences a good or bad film, this was partially why I chose not to do actors or directors because there would have been many examples where an actor or director involved once or twice and it would be hard to consider the impact in using the actor, director or country that is seen only once in this data set.

Finally, I picked out the key words of each film. Again, this follows the exact same methodology as the previous two features. The only difference is in this example I kept words used in at least five of the 250 films in order to have a value large enough where I could see the effect of the word while also limiting the the number of features so I will not over fit my model.

EDA, Model:

Once done, I was able to dummify my features in order to for each of the features, month, key words, and genre to be properly used.

One of the more interesting findings was how the number reviews was distributed. On the chart below, you can see how older movies (before 1990) had far fewer votes than more recent films.

For my model, I split the films into two categories, one for films with ratings lower than the collective median over the 250 films and the other under. I ran both a decision tree and a random forest in order to see which model would be more accurate. I trained my model using about 30% of the data and tested it on the remaining 70%. Ultimately, the decision tree had of about an 82% accuracy plus or minus 4.4 points while the random forest had an accuracy of  77.4% plus or minus 1.3 points. So the decision tree is more accurate but the random forest has less variance.

I then wanted to find out how much each feature weighed when it came to the mode. The number of votes and the year the film came out accounted for 29% and 11% respectively with months like October and November as well as genres like fantasy, thriller and drama having some impact. The problem with this is that in order to predict if something is good, then I cannot have future information. Because the votes come after the film is released, the number of votes is a feature that ruins the prediction element of the model and must be removed for the sake of fairness.

In redoing the model, the decision tree got only a 65% accuracy plus or minus 6.1 points while the random forest had a 70.9% accuracy plus or minus 6 points. SO in actuality the random forest is actaully the better model for this, I also wanted to see how important certain features became once I removed an element like number of votes that accounted for such a high percent of the importance. In doing so we see that year moves up to 15.7% while the next closest is October with less than 5%. This to me shows that there is a recency bias with films on IMDB. Essentially the higher up movies are generally the ones that have been seen more recently which makes sense because IMDB bases their top 250 films not solely on rating but on other factors as well.
